# Neural Network (NN)  

We will try to introduce the concepts of the **neural network** according to the [Keras](https://keras.io/) API.  

## Neuron  

The **neuron** is intrinsically to apply the **activation function** to the **logit** (namely, the **linear transform** of the inputs).  
 
For the **linear regression**, the **activation function** is the identity transform, and the **logit** is exactly the **predictions**.  

For the **logistic regression**, the **activation function** is the **sigmoid function**, and the **logit** is the **linear hyperplane** (**decision boundary**).  

### Layer  

The **layer** consists of several **neurons** of which the **activation function** is the same. The number of the **logits** within the **layer** is exactly the same as the number of the **neurons**.  

The **weights** of all **logits** within the layer is a N x U matrix (where the **bias** term is NOT included). The N is the length of one single input. The U is the number of the **neurons** in this **layer**. Each column of this N x U matrix presents the **weights** of each **neuron**.   

The **Param** returned by the **keras.Sequential.summary** is the number of all coefficients (where the **bias** term is included) of the **layers**. The **Param** of each **layer** is calculated as $\displaystyle Param = (N + 1) \times U$.  

```python
# Linear Regression
# We have 7 linear transforms of the inputs
tensorflow.keras.layers.Dense(units=7, activation="linear")

# Logistic Regression
# We have 7 linear hyperplanes (decision boundaries) of the inputs
# We apply the sigmoid function to each of these 7 linear hyperplanes
tensorflow.keras.layers.Dense(units=7, activation="sigmoid")

# Multinomial Logistic Regression
# We have 7 classes  
# We have 7 linear transforms (21 = 7 ร 6 รท 2 linear hyperplanes / decision boundaries) of the inputs  
# We apply the softmax function to the whole 7 linear tranforms of the inputs  
tensorflow.keras.layers.Dense(units=7, activation="sigmoid")
```

Note that we apply the **softmax** **activation function** to the **whole** 7 **linear transforms** of the inputs instead of **each** of them. This is different from both **linear** and **sigmoid** **activation functions**.  

## Model  

The **model** consists of several **layers**.  

The **intermediate layers** are called the **hidden layers**. The outputs of the **hidden layers** are called the **activation values** which do NOT exist in the training data but are generated by ourselves.  

The **final layer** is called the **output layer**. The outputs of the **output layers** are the **predictions**.  

The **features** can also be called the **input layer**. But the number of the **layers** of the **model** means the number of both the **hidden layers** and the **output layer** (where the **input layer** is NOT included).  

```python
tensorflow.keras.models.Sequential([
    # Hidden Layer 1
    # The "input_shape" defines the "input layer"
    tensorflow.keras.layers.Dense(units=25, activation="sigmoid", input_shape=(n,)),
    # Hidden Layer 2
    tensorflow.keras.layers.Dense(units=15, activation="sigmoid"),
    # Output Layer
    tensorflow.keras.layers.Dense(units=1, activation="sigmoid")
])
```  

## Notation  

We use $\displaystyle a^{[i]}$ to denote the **activation values** of the **hidden layer i**.  

We still use $\displaystyle a^{[0]}$ to denote the output values of the **input layer**, although the they are NOT the **activation values**.  

We use $\displaystyle \theta_j^{[i]}$ to denote the **coefficients** of j-th **logit** within the **hidden layer i**.  

For example, we have $\displaystyle a_j^{[2]} = \mathop{\mathrm{relu}}(\theta_j^{[i]} a^{[1]})$.  

The $\displaystyle a_j^{[2]}$ denotes the j-th (j = 1 2 3 ... 15) component of the **activation value** of the **hidden layer 2**.  

The $\displaystyle \theta_j^{[i]} a^{[1]}$ denotes the j-th **logit**  of **hidden layer 2**, and we apply the **activation function** **relu** to **each** of these 15 **logits** of the **hidden layer 2**.  

The $\displaystyle a^{[1]}$ denotes the **whole** output of the **hidden layer 1** of which the number of the components is 25.  

### Convolution Layer  
  
For **Dense** layer, the **input_shape** (namely, the shape of one single training example) should be the 1D vector. This means that we have the **logit** $\displaystyle Z = Wx + b$.  

However, for **Conv2D** layer, the **input_shape** can be the 2D (image).  

For example, we have 
```python
tensorflow.keras.layers.Conv2D(filters=F, kernel_size=(M,N), input_shape=(H,W))
```

The **F** denotes the number of the **filters**. 

For each pixel in W x H 2D image, we get the information of the vicinal M x N pixels and apply each of these F filters to them. This means that the shape of the **logit** of this **layer** is (W, H, F).  

The shape of these **filters** of this **layer** should be (M, N, D) of which the **D** is the depth of the input of this **layer**. If the input of this **layer** is from another **Conv2D** layer, the **D** should the **F** of that **layer**.  

### ReLU (Rectified Linear Unit) Activation Function  

The **hidden layer**, of which the **activation function** is **linear**, is equivalent to non-existence, since we can learn from mathematical that a linear function of a linear function is itself a linear function. This means that **linear** **activation function** should be avoided for the **hidden layer**.   

However, the **sigmoid** **activation function** is also NOT a good choice for the **hidden layer**. Evidently, the amount of calculation of the **sigmoid** **activation function** is much more than the **linear** **activation function**. At the same time, the **sigmoid** **activation function** is almost flat for some places. We can learn from intuition that the **cost funtion** will consequently be flat for some places. This means that the gradient will be close to zero for some places, and this makes the **gradient descent** NOT efficient.  

Thus, we introduce the **ReLU (Rectified Linear Unit)** **activation function** $\displaystyle a = \max (0, z)$ which is typically used for the **hidden layer**.  

This means that the previous model source can be changed as the following.  

```python
tensorflow.keras.models.Sequential([
    # Hidden Layer 1
    # The "input_shape" defines the "input layer"
    tensorflow.keras.layers.Dense(units=25, activation="relu", input_shape=(n,)),
    # Hidden Layer 2
    tensorflow.keras.layers.Dense(units=15, activation="relu"),
    # Output Layer
    tensorflow.keras.layers.Dense(units=1, activation="sigmoid")
])
```  

## Loss Function  
 
Since the **activion funtions** of the **hidden layers** are typically the **ReLU (Rectified Linear Unit)**, we only need to consider the **activation function** of the **output layer** to choose the consistent **loss function** (namely, the cost function).  
            
Technically, the **loss function** is decoupled from the **activation function**. This means that it is technically possible that the **MeanSquaredError** **loss function** is used with the **sigmoid** **activation function** or the **BinaryCrossentropy** **loss function** is used with the **linear** **activation function**. But this never happens in practical, since the consistent **loss function** of the **activation function** is deduced by the **MLE (Maximum Likelihood Estimation)** and should NOT be abused. The **loss function** should be consistent with the **activation function** of the **output layer** and the type of the **target** of the **training examples**.  

Actually, based on the **chain rule** from mathematical, the tensorflow uses the **backward propagation** to calculate the gradient of each **neuron** to avoid redundant calculation. But the technique detail will NOT be involved here.  

We can use the following source to complie the model and train the model by **gradient descent**.  

```python
# Specify the Gradient Descent Optimizer and the Binary Cross Entropy Loss Function
keras_model.compile(optimizer=tensorflow.keras.optimizers.SGD(learning_rate=alpha), loss=tensorflow.keras.losses.BinaryCrossentropy)

# The Gradient Descent Loop
keras_history = keras_model.fit(X, y, epochs=iteration_count, callbacks=[ tensorflow.keras.callbacks.EarlyStopping(monitor='loss', min_delta=gamma)])
```

However, the more efficient approach is to output the **logits** of the **output layer** directly and enable the **from_logits=True** of the **loss function**.  In this way, the output of the **sigmoid** **activation function** is hidden from the user and can be treated as the **intermediate** values by the tensorflow. This means that the tensorflow can do more optimization. And the float numerical roundoff errors can also benifit from this optimization.  

This means that the previous source can be changed as the following.  

```python
tensorflow.keras.models.Sequential([
    # Hidden Layer 1
    # The "input_shape" defines the "input layer"
    tensorflow.keras.layers.Dense(units=25, activation="relu", input_shape=(n,)),
    # Hidden Layer 2
    tensorflow.keras.layers.Dense(units=15, activation="relu"),
    # Output Layer
    # We output the **logits** of the **output layer** directly
    tensorflow.keras.layers.Dense(units=1, activation="linear")
])

# We enable the "from_logits=True" of the loss function
keras_model.compile(optimizer=tensorflow.keras.optimizers.SGD(learning_rate=alpha), loss=tensorflow.keras.losses.BinaryCrossentropy(from_logits=True))
```  

When we perform the **inference**, we do NOT need to calcucalte the **sigmod** **activation function**.  According to the knowledge of the **decision boundary** of the logistic regression, we only need to check whether the **logits** are greater than zero.  

```python
# Inference
logit_prediction = keras_model.predict(X)

prediction = (logit_prediction > 0.0).flatten().astype(int)
```  

Actually, we also do NOT need to calculate the **softmax** **activation function**. We only need to find the maximum index of the **logits**.  

```python
# Inference
logit_prediction = keras_model.predict(X_test)

prediction = numpy.argmax(logit_prediction, axis=1)
```

// ===============

// note that the **input_shape=(n,)** parameter of the keras layer is reversed the first number is the demension of the feature (the column) the second number is the number of the training examples and is  usually ommited  

// note in numpy (n,) denotes one demension vector 1 ร n // n denotes column but comes first  

// numpy broadcast: when add "2D (1,j)" or "1D (j,)" to (m, j), broadcast to each row (tatally m rows)  
 
// actually, we can add keras.Input into the sequential // but it is more convenient to define "input_shape=" parameter for the first layer  

// cost history  
// the data from the return "history" of model.fit  

kernel_regularization=L2(lambda)   
 
m - row - number of the training examples  
n - column - the feature vector of one single training example  

// Inference  

forward propagation  

// Train  

if the coefficients (kernel_initializer and bias_initializer) are initialized the same values, the training results of the neurals within the same layer should be exactly the same due to the symmetry  

Usually, we use random initial values to prevent redundancy. This makes it difficult to reproduce the training results of the machine learning algorithm.  


